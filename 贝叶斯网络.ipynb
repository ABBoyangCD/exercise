{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户评论分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建词汇表\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],                #切分的词条\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "classVec = [0,1,0,1,0,1]                                                         #类别标签向量，1代表侮辱性词汇，0代表不是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对词条向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myVocabList:\n",
      " ['food', 'please', 'dalmation', 'posting', 'stupid', 'park', 'so', 'mr', 'to', 'love', 'stop', 'help', 'garbage', 'maybe', 'is', 'not', 'ate', 'quit', 'dog', 'take', 'my', 'cute', 'buying', 'flea', 'I', 'him', 'problems', 'steak', 'how', 'has', 'worthless', 'licks']\n",
      "trainMat:\n",
      " [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "#词条转换为词条向量\n",
    "def createVocabList(postingList):\n",
    "    vocabSet= set(word for senten in postingList for word in senten) #取集合，去重复\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)                                    #创建一个其中所含元素都为0的向量\n",
    "    for word in inputSet:                                                #遍历每个词条\n",
    "        if word in vocabList:                                            #如果词条存在于词汇表中，则置1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec                                                    #返回文档向量\n",
    " \n",
    "\n",
    "myVocabList = createVocabList(postingList)#词汇表，所有单词出现的集合\n",
    "print('myVocabList:\\n',myVocabList)\n",
    "trainMat = []\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))#词条向量化\n",
    "print('trainMat:\\n', trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>please</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>posting</th>\n",
       "      <th>stupid</th>\n",
       "      <th>park</th>\n",
       "      <th>so</th>\n",
       "      <th>mr</th>\n",
       "      <th>to</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>flea</th>\n",
       "      <th>I</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>steak</th>\n",
       "      <th>how</th>\n",
       "      <th>has</th>\n",
       "      <th>worthless</th>\n",
       "      <th>licks</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   food  please  dalmation  posting  stupid  park  so  mr  to  love  ...  \\\n",
       "0     0       1          0        0       0     0   0   0   0     0  ...   \n",
       "1     0       0          0        0       1     1   0   0   1     0  ...   \n",
       "2     0       0          1        0       0     0   1   0   0     1  ...   \n",
       "3     0       0          0        1       1     0   0   0   0     0  ...   \n",
       "4     0       0          0        0       0     0   0   1   1     0  ...   \n",
       "5     1       0          0        0       1     0   0   0   0     0  ...   \n",
       "\n",
       "   flea  I  him  problems  steak  how  has  worthless  licks  label  \n",
       "0     1  0    0         1      0    0    1          0      0      0  \n",
       "1     0  0    1         0      0    0    0          0      0      1  \n",
       "2     0  1    1         0      0    0    0          0      0      0  \n",
       "3     0  0    0         0      0    0    0          1      0      1  \n",
       "4     0  0    1         0      1    1    0          0      1      0  \n",
       "5     0  0    0         0      0    0    0          1      0      1  \n",
       "\n",
       "[6 rows x 33 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(trainMat,columns=myVocabList)\n",
    "df['label']=classVec\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求H0和H1的先验概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph1=df['label'].sum()/len(df['label'])\n",
    "ph1 #词条类别为1的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph0=1-ph1\n",
    "ph0 #词条类别为0的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>please</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>posting</th>\n",
       "      <th>stupid</th>\n",
       "      <th>park</th>\n",
       "      <th>so</th>\n",
       "      <th>mr</th>\n",
       "      <th>to</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>flea</th>\n",
       "      <th>I</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>steak</th>\n",
       "      <th>how</th>\n",
       "      <th>has</th>\n",
       "      <th>worthless</th>\n",
       "      <th>licks</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   food  please  dalmation  posting  stupid  park  so  mr  to  love  ...  \\\n",
       "0     0       1          0        0       0     0   0   0   0     0  ...   \n",
       "2     0       0          1        0       0     0   1   0   0     1  ...   \n",
       "4     0       0          0        0       0     0   0   1   1     0  ...   \n",
       "\n",
       "   flea  I  him  problems  steak  how  has  worthless  licks  label  \n",
       "0     1  0    0         1      0    0    1          0      0      0  \n",
       "2     0  1    1         0      0    0    0          0      0      0  \n",
       "4     0  0    1         0      1    1    0          0      1      0  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0=df[df['label']==0]#选择标签为0的所有样本\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>please</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>posting</th>\n",
       "      <th>stupid</th>\n",
       "      <th>park</th>\n",
       "      <th>so</th>\n",
       "      <th>mr</th>\n",
       "      <th>to</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>flea</th>\n",
       "      <th>I</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>steak</th>\n",
       "      <th>how</th>\n",
       "      <th>has</th>\n",
       "      <th>worthless</th>\n",
       "      <th>licks</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   food  please  dalmation  posting  stupid  park  so  mr  to  love  ...  \\\n",
       "1     0       0          0        0       1     1   0   0   1     0  ...   \n",
       "3     0       0          0        1       1     0   0   0   0     0  ...   \n",
       "5     1       0          0        0       1     0   0   0   0     0  ...   \n",
       "\n",
       "   flea  I  him  problems  steak  how  has  worthless  licks  label  \n",
       "1     0  0    1         0      0    0    0          0      0      1  \n",
       "3     0  0    0         0      0    0    0          1      0      1  \n",
       "5     0  0    0         0      0    0    0          1      0      1  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df[df['label']==1]#选择标签为1的所有样本\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.33333333, 0.33333333, 0.        , 0.        ,\n",
       "       0.        , 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
       "       0.33333333, 0.33333333, 0.        , 0.        , 0.33333333,\n",
       "       0.        , 0.33333333, 0.        , 0.33333333, 0.        ,\n",
       "       1.        , 0.33333333, 0.        , 0.33333333, 0.33333333,\n",
       "       0.66666667, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
       "       0.        , 0.33333333])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V=np.array(df0.iloc[:,0:-1].sum(axis=0))/len(df0)# p(x|h0)\n",
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.        , 0.        , 0.33333333, 1.        ,\n",
       "       0.33333333, 0.        , 0.        , 0.33333333, 0.        ,\n",
       "       0.33333333, 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "       0.33333333, 0.        , 0.33333333, 0.66666667, 0.33333333,\n",
       "       0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "       0.33333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.66666667, 0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V=np.array(df1.iloc[:,0:-1].sum(axis=0))/len(df1)# p(x|h1)\n",
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>please</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>posting</th>\n",
       "      <th>stupid</th>\n",
       "      <th>park</th>\n",
       "      <th>so</th>\n",
       "      <th>mr</th>\n",
       "      <th>to</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>buying</th>\n",
       "      <th>flea</th>\n",
       "      <th>I</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>steak</th>\n",
       "      <th>how</th>\n",
       "      <th>has</th>\n",
       "      <th>worthless</th>\n",
       "      <th>licks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       food    please  dalmation   posting  stupid      park        so  \\\n",
       "0  0.000000  0.333333   0.333333  0.000000     0.0  0.000000  0.333333   \n",
       "1  0.333333  0.000000   0.000000  0.333333     1.0  0.333333  0.000000   \n",
       "\n",
       "         mr        to      love  ...    buying      flea         I       him  \\\n",
       "0  0.333333  0.333333  0.333333  ...  0.000000  0.333333  0.333333  0.666667   \n",
       "1  0.000000  0.333333  0.000000  ...  0.333333  0.000000  0.000000  0.333333   \n",
       "\n",
       "   problems     steak       how       has  worthless     licks  \n",
       "0  0.333333  0.333333  0.333333  0.333333   0.000000  0.333333  \n",
       "1  0.000000  0.000000  0.000000  0.000000   0.666667  0.000000  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([p0V,p1V],columns=myVocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。 为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题  \n",
    "2.下溢出问题：太多很小的数相乘，导致下溢出，结果可能变成零。 为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算法的改进1，分子加1，分母加2\n",
    "p1V=(np.array(df1.iloc[:,0:-1].sum(axis=0))+1)/(len(df1)+2)\n",
    "p0V=(np.array(df0.iloc[:,0:-1].sum(axis=0))+1)/(len(df0)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>please</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>posting</th>\n",
       "      <th>stupid</th>\n",
       "      <th>park</th>\n",
       "      <th>so</th>\n",
       "      <th>mr</th>\n",
       "      <th>to</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>buying</th>\n",
       "      <th>flea</th>\n",
       "      <th>I</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>steak</th>\n",
       "      <th>how</th>\n",
       "      <th>has</th>\n",
       "      <th>worthless</th>\n",
       "      <th>licks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   food  please  dalmation  posting  stupid  park   so   mr   to  love  ...  \\\n",
       "0   0.2     0.4        0.4      0.2     0.2   0.2  0.4  0.4  0.4   0.4  ...   \n",
       "1   0.4     0.2        0.2      0.4     0.8   0.4  0.2  0.2  0.4   0.2  ...   \n",
       "\n",
       "   buying  flea    I  him  problems  steak  how  has  worthless  licks  \n",
       "0     0.2   0.4  0.4  0.6       0.4    0.4  0.4  0.4        0.2    0.4  \n",
       "1     0.4   0.2  0.2  0.4       0.2    0.2  0.2  0.2        0.6    0.2  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([p0V,p1V],columns=myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算法的改进2，取log\n",
    "p1V=np.log(p1V)\n",
    "p0V=np.log(p0V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>please</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>posting</th>\n",
       "      <th>stupid</th>\n",
       "      <th>park</th>\n",
       "      <th>so</th>\n",
       "      <th>mr</th>\n",
       "      <th>to</th>\n",
       "      <th>love</th>\n",
       "      <th>...</th>\n",
       "      <th>buying</th>\n",
       "      <th>flea</th>\n",
       "      <th>I</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>steak</th>\n",
       "      <th>how</th>\n",
       "      <th>has</th>\n",
       "      <th>worthless</th>\n",
       "      <th>licks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.916291</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-1.609438</td>\n",
       "      <td>-0.510826</td>\n",
       "      <td>-1.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       food    please  dalmation   posting    stupid      park        so  \\\n",
       "0 -1.609438 -0.916291  -0.916291 -1.609438 -1.609438 -1.609438 -0.916291   \n",
       "1 -0.916291 -1.609438  -1.609438 -0.916291 -0.223144 -0.916291 -1.609438   \n",
       "\n",
       "         mr        to      love  ...    buying      flea         I       him  \\\n",
       "0 -0.916291 -0.916291 -0.916291  ... -1.609438 -0.916291 -0.916291 -0.510826   \n",
       "1 -1.609438 -0.916291 -1.609438  ... -0.916291 -1.609438 -1.609438 -0.916291   \n",
       "\n",
       "   problems     steak       how       has  worthless     licks  \n",
       "0 -0.916291 -0.916291 -0.916291 -0.916291  -1.609438 -0.916291  \n",
       "1 -1.609438 -1.609438 -1.609438 -1.609438  -0.510826 -1.609438  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([p0V,p1V],columns=myVocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非侮辱类\n"
     ]
    }
   ],
   "source": [
    "#测试1\n",
    "test = ['love', 'my', 'dalmation'] \n",
    "testEntry=setOfWords2Vec(myVocabList, test)\n",
    " \n",
    "p1=(p1V*testEntry).sum()+np.log(ph1)  #为1类的概率\n",
    "\n",
    "\n",
    "p0=(p0V*testEntry).sum()+np.log(ph0)  #为0类的概率\n",
    "if p1 > p0:\n",
    "    print('侮辱类')\n",
    "else:\n",
    "    print('非侮辱类')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        , -0.        , -1.60943791, -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -1.60943791,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -1.60943791, -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V*testEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "侮辱类\n"
     ]
    }
   ],
   "source": [
    "#测试2\n",
    "test = ['stupid', 'garbage']     \n",
    "testEntry=setOfWords2Vec(myVocabList, test)\n",
    "p1=(p1V*testEntry).sum()+np.log(ph1)\n",
    "p0=(p0V*testEntry).sum()+np.log(ph0)\n",
    "if p1 > p0:\n",
    "    print('侮辱类')\n",
    "else:\n",
    "    print('非侮辱类')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非侮辱类\n"
     ]
    }
   ],
   "source": [
    "#完整代码\n",
    "#训练集，标签\n",
    "postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],                #切分的词条\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "classVec = [0,1,0,1,0,1]\n",
    "\n",
    "#向量化操作\n",
    "trainMat = []\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))#词条向量化\n",
    "\n",
    "df=pd.DataFrame(trainMat,columns=myVocabList)\n",
    "df['label']=classVec\n",
    "\n",
    "df0=df[df['label']==0]#选择标签为0的\n",
    "df1=df[df['label']==1]#选择标签为0的\n",
    "\n",
    "#算法的改进1，分子加1，分母加2\n",
    "p1V=(np.array(df1.iloc[:,0:-1].sum(axis=0))+1)/(len(df1)+2)\n",
    "p0V=(np.array(df0.iloc[:,0:-1].sum(axis=0))+1)/(len(df0)+2)\n",
    "#算法的改进2，取log\n",
    "p1V=np.log(p1V)\n",
    "p0V=np.log(p0V)\n",
    "                                                \n",
    "#测试\n",
    "test = ['love', 'my', 'dalmation'] \n",
    "testEntry=setOfWords2Vec(myVocabList, test)\n",
    "p1=(p1V*testEntry).sum()+np.log(ph1)#为1类的概率\n",
    "p0=(p0V*testEntry).sum()+np.log(ph0)#为0类的概率\n",
    "if p1 > p0:\n",
    "    print('侮辱类')\n",
    "else:\n",
    "    print('非侮辱类')                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x19 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "#语料，词条\n",
    "corpus=[\"I come to  to  to China travel travel travel\", \n",
    "    \"This is a car to polupar in China\",          \n",
    "    \"I love tea and to Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "vectorizer=CountVectorizer()\n",
    "vectorizer.fit_transform(corpus) #稀疏矩阵形式进行存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>apple</th>\n",
       "      <th>car</th>\n",
       "      <th>china</th>\n",
       "      <th>come</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>papers</th>\n",
       "      <th>polupar</th>\n",
       "      <th>science</th>\n",
       "      <th>some</th>\n",
       "      <th>tea</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>travel</th>\n",
       "      <th>work</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  apple  car  china  come  in  is  love  papers  polupar  science  some  \\\n",
       "0    0      0    0      1     1   0   0     0       0        0        0     0   \n",
       "1    0      0    1      1     0   1   1     0       0        1        0     0   \n",
       "2    1      1    0      0     0   0   0     1       0        0        0     0   \n",
       "3    0      0    0      0     0   1   1     0       1        0        1     1   \n",
       "\n",
       "   tea  the  this  to  travel  work  write  \n",
       "0    0    0     0   3       3     0      0  \n",
       "1    0    0     1   1       0     0      0  \n",
       "2    1    0     0   1       0     0      0  \n",
       "3    0    1     0   1       0     1      1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(vectorizer.fit_transform(corpus).toarray(),columns=vectorizer.get_feature_names()) #自动去掉停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x19 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus=[\"I come to  to  to China travel travel travel\", \n",
    "    \"This is a car to polupar in China\",          \n",
    "    \"I love tea and to Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "tfidf = TfidfVectorizer()\n",
    "re = tfidf.fit_transform(corpus)\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>apple</th>\n",
       "      <th>car</th>\n",
       "      <th>china</th>\n",
       "      <th>come</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>papers</th>\n",
       "      <th>polupar</th>\n",
       "      <th>science</th>\n",
       "      <th>some</th>\n",
       "      <th>tea</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>travel</th>\n",
       "      <th>work</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218059</td>\n",
       "      <td>0.27658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432993</td>\n",
       "      <td>0.829741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>0.347852</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.347852</td>\n",
       "      <td>0.347852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>0.230239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.287590</td>\n",
       "      <td>0.287590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>0.364772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and     apple       car     china     come        in        is  \\\n",
       "0  0.000000  0.000000  0.000000  0.218059  0.27658  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.441206  0.347852  0.00000  0.347852  0.347852   \n",
       "2  0.483803  0.483803  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.00000  0.287590  0.287590   \n",
       "\n",
       "       love    papers   polupar   science      some       tea       the  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.441206  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.483803  0.000000  0.000000  0.000000  0.000000  0.483803  0.000000   \n",
       "3  0.000000  0.364772  0.000000  0.364772  0.364772  0.000000  0.364772   \n",
       "\n",
       "       this        to    travel      work     write  \n",
       "0  0.000000  0.432993  0.829741  0.000000  0.000000  \n",
       "1  0.441206  0.230239  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.252468  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.190353  0.000000  0.364772  0.364772  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF模型\n",
    "import pandas as pd\n",
    "pd.DataFrame(re.toarray(),columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['结婚', '的', '和', '尚未', '结婚', '的']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba  #结巴\n",
    "jieba.lcut('结婚的和尚未结婚的') #分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'结婚 的 和 尚未 结婚 的'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(jieba.lcut('结婚的和尚未结婚的'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我 喜欢 去 中国 旅游', '这个 车 在 中国 很 流行', '我 喜欢 茶叶 和 苹果', '这个 工作 是 写 一些 科技 文献']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba  #结巴\n",
    "corpus=[\"我喜欢去中国旅游\", \n",
    "    \"这个车在中国很流行\",          \n",
    "    \"我喜欢茶叶和苹果\",   \n",
    "    \"这个工作是写一些科技文献\"] \n",
    "newcorpus=[' '.join(jieba.lcut(sen)) for sen in corpus]\n",
    "newcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一些</th>\n",
       "      <th>中国</th>\n",
       "      <th>喜欢</th>\n",
       "      <th>工作</th>\n",
       "      <th>文献</th>\n",
       "      <th>旅游</th>\n",
       "      <th>流行</th>\n",
       "      <th>科技</th>\n",
       "      <th>苹果</th>\n",
       "      <th>茶叶</th>\n",
       "      <th>这个</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   一些  中国  喜欢  工作  文献  旅游  流行  科技  苹果  茶叶  这个\n",
       "0   0   1   1   0   0   1   0   0   0   0   0\n",
       "1   0   1   0   0   0   0   1   0   0   0   1\n",
       "2   0   0   1   0   0   0   0   0   1   1   0\n",
       "3   1   0   0   1   1   0   0   1   0   0   1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "pd.DataFrame(vectorizer.fit_transform(newcorpus).toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn中的贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "seed=10\n",
    "X = np.random.randint(5, size=(10, 10))\n",
    "y = np.array([1, 2, 3, 4, 5, 6, 1, 2, 3, 4])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[1,1,1,1,1,1,1,1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 4, 3, 4, 1, 4, 4, 1, 0],\n",
       "       [3, 0, 1, 2, 1, 4, 3, 3, 1, 3],\n",
       "       [4, 0, 4, 4, 2, 1, 4, 2, 3, 2],\n",
       "       [3, 1, 3, 4, 4, 0, 1, 0, 3, 0],\n",
       "       [4, 0, 0, 3, 3, 0, 4, 0, 3, 4],\n",
       "       [0, 4, 2, 3, 2, 0, 4, 2, 4, 4],\n",
       "       [0, 2, 0, 1, 3, 2, 4, 1, 4, 4],\n",
       "       [0, 1, 0, 3, 4, 2, 2, 0, 3, 1],\n",
       "       [1, 4, 4, 0, 4, 3, 0, 3, 2, 1],\n",
       "       [1, 2, 1, 3, 1, 2, 0, 2, 4, 4]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26182584, 0.06671389, 0.37091454, 0.16064312, 0.09667629,\n",
       "        0.04322631]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba([[1,1,1,1,1,1,1,1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 伯努利模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "X = np.random.randint(2, size=(6, 10))\n",
    "Y = np.array([1, 2, 3, 4, 4, 5])\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯模型，鸢尾花分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入算法包以及数据集\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "iris = datasets.load_iris()\n",
    "x_train,x_test,y_train,y_test = train_test_split(iris.data, iris.target,random_state=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_nb = GaussianNB()\n",
    "mul_nb.fit(x_train,y_train)#训练，拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 2, 1, 0, 0, 2, 1, 0, 0, 0, 2, 2,\n",
       "       2, 0, 1, 0, 1, 1, 1, 2, 1, 1, 2, 2, 2, 0, 2, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_nb.predict(x_test)#预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul_nb.score(x_test,y_test)#模型得分，准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,mul_nb.predict(x_test)))#混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,mul_nb.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文新闻分类，20newsgroups\n",
    "20 newsgroups数据集18000篇新闻文章，一共涉及到20种话题，所以称作20 newsgroups text dataset，分为两部分：训练集和测试集，通常用来做文本分类."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch_20newsgroups的参数设置：\n",
    "fetch_20newsgroups(data_home=None, # 文件下载的路径\n",
    "                   subset='train', # 加载那一部分数据集 train/test\n",
    "                   categories=None, # 选取哪一类数据集[类别列表]，默认20类\n",
    "                   shuffle=True,  # 将数据集随机排序\n",
    "                   random_state=42, # 随机数生成器\n",
    "                   remove=(), # ('headers','footers','quotes') 去除部分文本\n",
    "                   download_if_missing=True # 如果没有下载过，重新下载\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\CDA\\File')\n",
    "from sklearn.datasets import fetch_20newsgroups  #20newsbydate.tar.gz\n",
    "news = fetch_20newsgroups(data_home=r'D:\\CDA\\File',subset='all' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news.data) #总的文本数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.data[0]#第一篇文章内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target #类别编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target.shape#总数,label数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names#新闻的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18846x173762 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2952534 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将文本转为TF-IDF向量\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 提取tfidf特征\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(news.data)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x173762 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 110 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0,:]#稀疏矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集和测试集分别处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train = fetch_20newsgroups(data_home=r'D:\\CDA\\File',subset='train' ) #训练集\n",
    "news_test = fetch_20newsgroups(data_home=r'D:\\CDA\\File',subset='test' )   #测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors_train = vectorizer.fit_transform(news_train.data)   #训练集特征提取\n",
    "vectors_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7532x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1107956 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_test = vectorizer.transform(news_test.data)         #测试集特征提取\n",
    "vectors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#简单的测试模型\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(vectors_train, news_train.target)\n",
    "nb.score(vectors_test, news_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化参数，网格搜索优化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01020408163265306}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "nb = MultinomialNB()\n",
    "params={'alpha':np.linspace(0,0.5,50)}\n",
    "grid_search=GridSearchCV(nb,param_grid=params,cv=5,verbose=2,n_jobs=-1)\n",
    "grid_search.fit(vectors_train, news_train.target)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8352363250132767"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(vectors_test, news_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增加停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'are',\n",
       " 'area',\n",
       " 'around',\n",
       " 'as',\n",
       " 'asked',\n",
       " 'at',\n",
       " 'away',\n",
       " 'back',\n",
       " 'be',\n",
       " 'because',\n",
       " 'become',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'being',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'both',\n",
       " 'business',\n",
       " 'but',\n",
       " 'by',\n",
       " 'camQ',\n",
       " 'can',\n",
       " 'case',\n",
       " 'certain',\n",
       " 'come',\n",
       " 'could',\n",
       " 'day',\n",
       " 'did',\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " 'done',\n",
       " 'door',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'early',\n",
       " 'end',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'family',\n",
       " 'far',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'find',\n",
       " 'first',\n",
       " 'for',\n",
       " 'four',\n",
       " 'from',\n",
       " 'general',\n",
       " 'get',\n",
       " 'give',\n",
       " 'given',\n",
       " 'go',\n",
       " 'god',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'group',\n",
       " 'had',\n",
       " 'hand',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'head',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'high',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'home',\n",
       " 'house',\n",
       " 'how',\n",
       " 'however',\n",
       " 'i',\n",
       " 'if',\n",
       " 'important',\n",
       " 'in',\n",
       " 'interest',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'kind',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'large',\n",
       " 'last',\n",
       " 'later',\n",
       " 'least',\n",
       " 'less',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'long',\n",
       " 'mE',\n",
       " 'made',\n",
       " 'make',\n",
       " 'man',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'members',\n",
       " 'men',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'national',\n",
       " 'need',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'night',\n",
       " 'no',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'number',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'or',\n",
       " 'order',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'people',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'place',\n",
       " 'point',\n",
       " 'possible',\n",
       " 'power',\n",
       " 'present',\n",
       " 'problem',\n",
       " 'program',\n",
       " 'public',\n",
       " 'put',\n",
       " 'rather',\n",
       " 'right',\n",
       " 'room',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'school',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seemed',\n",
       " 'sense',\n",
       " 'set',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'side',\n",
       " 'since',\n",
       " 'small',\n",
       " 'so',\n",
       " 'social',\n",
       " 'some',\n",
       " 'something',\n",
       " 'state',\n",
       " 'states',\n",
       " 'still',\n",
       " 'such',\n",
       " 'system',\n",
       " 'take',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'through',\n",
       " 'thus',\n",
       " 'time',\n",
       " 'to',\n",
       " 'today',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'turned',\n",
       " 'two',\n",
       " 'under',\n",
       " 'united',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'very',\n",
       " 'want',\n",
       " 'war',\n",
       " 'was',\n",
       " 'water',\n",
       " 'way',\n",
       " 'we',\n",
       " 'we re',\n",
       " 'well',\n",
       " 'went',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'white',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'work',\n",
       " 'world',\n",
       " 'would',\n",
       " 'year',\n",
       " 'years',\n",
       " 'you',\n",
       " 'young',\n",
       " 'your'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#添加停用词\n",
    "def get_stop_words():\n",
    "    result = set()\n",
    "    for line in open(r'D:\\CDA\\File\\stopwords_en.txt','r').readlines():\n",
    "        result.add(line.strip())\n",
    "    return result\n",
    "# 加载停用词\n",
    "stop_words = get_stop_words()\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['camq', 're'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x129834 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1248887 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectors_train = vectorizer.fit_transform(news_train.data)\n",
    "vectors_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_test = vectorizer.transform(news_test.data)         #测试集特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   18.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.030612244897959183}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "nb = MultinomialNB()\n",
    "params={'alpha':np.linspace(0,0.5,50)}\n",
    "grid_search=GridSearchCV(nb,param_grid=params,cv=5,verbose=2,n_jobs=-1)\n",
    "grid_search.fit(vectors_train, news_train.target)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8372278279341476"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(vectors_test, news_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "中文处理流程：\n",
    "1、分词，转换为用空格分开的字符串列表\n",
    "2、加载停用词\n",
    "3、向量化\n",
    "4、建模\n",
    "5、调参\n",
    "6、测试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.638px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
